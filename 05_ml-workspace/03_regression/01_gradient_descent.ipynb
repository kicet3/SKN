{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사하강법 (Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 잔차제곱합 (Residual Sum of Squares, RSS)\n",
    "- 잔차 = 실제 값 - 예측 값\n",
    "- 잔차제곱합 = (실제 값 - 예측 값)의 제곱의 합\n",
    "- 회귀 모델의 정확도를 측정하는 지표\n",
    "    - RSS가 작을수록 정확하게 예측하는 모델\n",
    "    - RSS가 클수록 잘못된 예측하는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 회귀 모델은 RSS가 최소가 되는 방향으로 학습이 진행됨 = 회귀계수(절편)는 RSS가 최소가 되도록 학습\n",
    "- 비용함수 R(w)가 가장 작을 때의 w를 찾는 것이 회귀 모델의 목표\n",
    "    - 매 회차에 계산된 R(w)에서 순간변화율(기울기)를 구해야 함 -> 미분 사용\n",
    "    - 단, 우리가 구해야 하는 회귀계수는 하나 이상이므로 우리는 편미분을 사용함\n",
    "        - w0(절편)을 고정한 채로 w1의 미분을 구하고, w1을 고정한 채로 w0 미분을 구함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습률 (Learning Rate)\n",
    "- 최적의 해를 빠르게 혹은 천천히 조금씩 찾아가는 '정도'를 가르키는 하이퍼 파라미터\n",
    "- 기본 값으로 보통 0.001을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**경사하강법 수식**\n",
    "\n",
    "$w_1$ $w_0$을 반복적으로 업데이트하며 최적의 회귀계수를 찾음\n",
    "<br/>\n",
    "$w_1 = w_1 - (-η\\frac{2}{N}\\sum^{N}_{i=1} x_i * (실제값_i - 예측값_i))$\n",
    "<br/>\n",
    "$w_0 = w_0 - (-η\\frac{2}{N}\\sum^{N}_{i=1}(실제값_i - 예측값_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**경사하강법 공식**\n",
    "\n",
    "$w1 = w1 - (미분값)$\n",
    "\n",
    "$w1 = w1 - (-학습률 * 2 / N * (x * (실제값 - 예측값))의 합)$\n",
    "\n",
    "$w0 = w0 - (미분값)$\n",
    "\n",
    "$w0 = w0 - (-학습률 * 2 / N * (실제값 - 예측값)의 합)$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
