{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기울기 소실 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "np.random.seed(0)\n",
    "weights = np.random.randn(10, 10) * 0.01\n",
    "x = np.random.randn(10, 1)\n",
    "\n",
    "for i in range(1, 11):\n",
    "    x = sigmoid(np.dot(weights, x))\n",
    "    print(f'{i}번째 층 출력 평균: {np.mean(x)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "for i in range(1, 6):\n",
    "    x = sigmoid(x)\n",
    "    plt.plot(x, label=f'{i} layer')\n",
    "\n",
    "plt.xlabel('input')\n",
    "plt.ylabel('output')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 활성화 함수 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return sigmoid(x) * (1- sigmoid(x))\n",
    "\n",
    "def relu_grad(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(x, sigmoid_grad(x), label=\"Sigmoid Gradient\")\n",
    "plt.plot(x, relu_grad(x), label=\"ReLU Gradient\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('gradient')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 잔차 연결(Residual Connection) 효과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(10, 10)\n",
    "        self.layer2 = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer2(torch.relu(self.layer1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResidualNN, self).__init__()\n",
    "        self.layer = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + torch.relu(self.layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperResidualNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeeperResidualNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(10, 10)\n",
    "        self.layer2 = nn.Linear(10, 10)\n",
    "        self.layer3 = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = SimpleNN()\n",
    "residual_model = ResidualNN()\n",
    "\n",
    "x = torch.randn(1, 10)\n",
    "\n",
    "output_simple = simple_model(x)\n",
    "output_residual = residual_model(x)\n",
    "\n",
    "print(f'입력값-일반 신경망: {x - output_simple}')\n",
    "print(f'입력값-잔차 연결 신경망: {x - output_residual}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기 변화\n",
    "\n",
    "# 데이터 생성\n",
    "x = torch.randn(100, 10) \n",
    "y = torch.randn(100, 10)\n",
    "\n",
    "# 모델 생성\n",
    "sim_model = SimpleNN()\n",
    "res_model = ResidualNN()\n",
    "deep_res_model = DeeperResidualNN()\n",
    "\n",
    "# 손실함수, 옵티마이저 생성\n",
    "criterion = nn.MSELoss()\n",
    "sim_optim = optim.Adam(sim_model.parameters(), lr=0.01)\n",
    "res_optim = optim.Adam(res_model.parameters(), lr=0.01)\n",
    "deep_res_optim = optim.Adam(deep_res_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프\n",
    "epochs = 100\n",
    "loss_sim_list = []\n",
    "loss_res_list = []\n",
    "deep_res_loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    sim_optim.zero_grad()\n",
    "    sim_output = sim_model(x)\n",
    "    sim_loss = criterion(sim_output, y)\n",
    "    sim_loss.backward()\n",
    "    sim_optim.step()\n",
    "    loss_sim_list.append(sim_loss.item())\n",
    "\n",
    "    res_optim.zero_grad()\n",
    "    res_output = res_model(x)\n",
    "    res_loss = criterion(res_output, y)\n",
    "    res_loss.backward()\n",
    "    res_optim.step()\n",
    "    loss_res_list.append(res_loss.item())\n",
    "\n",
    "    deep_res_optim.zero_grad()\n",
    "    deep_res_output = deep_res_model(x)\n",
    "    deep_res_loss = criterion(deep_res_output, y)\n",
    "    deep_res_loss.backward()\n",
    "    deep_res_optim.step()\n",
    "    deep_res_loss_list.append(deep_res_loss.item())\n",
    "\n",
    "print(f'일반 신경망 최종 손실값: {loss_sim_list[-1]:.4f}')\n",
    "print(f'잔차 연결 신경망 최종 손실값: {loss_res_list[-1]:.4f}')\n",
    "print(f'더 깊은 잔차 연결 신경망 최종 손실값: {deep_res_loss_list[-1]:.4f}')\n",
    "\n",
    "for param in sim_model.parameters():\n",
    "    print(f'일반 신경망 기울기 크기: {param.grad.norm()}')\n",
    "\n",
    "for param in res_model.parameters():\n",
    "    print(f'잔차 연결 신경망 기울기 크기: {param.grad.norm()}')\n",
    "\n",
    "for param in deep_res_model.parameters():\n",
    "    print(f'더 깊은 잔차 연결 신경망 기울기 크기: {param.grad.norm()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
